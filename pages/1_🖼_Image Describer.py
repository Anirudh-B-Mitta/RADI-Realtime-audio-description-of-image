import streamlit as st
from PIL import Image
from helper_fcns import describer, audio_maker, translator

logo = Image.open("res/logo_V2.png")

st.set_page_config(
    page_title= "Image Describer",
    page_icon= logo
)

st.image(Image.open("res/logo_V1.png"), width=75)
st.markdown("# ðŸ–¼ Image Describer")
st.sidebar.markdown("# Image Describer")

st.write("\n")
uploaded_image = st.file_uploader("Upload an Image to be described:",type=["jpeg", "png", "jpg"])
lang = st.selectbox("Language: ",['English', 'Kannada', 'Hindi', 'Telugu', 'Urdu'])

if uploaded_image != None:
    if st.button("Submit"):
        try:
            uploaded_image = Image.open(uploaded_image)
            uploaded_image.save("res/temp.png")
            st.image(uploaded_image, width=200)
            text = describer.describer()
            if(lang == 'English'):
                audio_maker.make_aud(text,'en')
            elif(lang == 'Kannada'):
                text = translator.translator(text,'kn')
                audio_maker.make_aud(text,'kn')
            elif(lang == 'Hindi'):
                text = translator.translator(text,'hi')
                audio_maker.make_aud(text,'hi')
            elif(lang == 'Telugu'):
                text = translator.translator(text,'te')
                audio_maker.make_aud(text,'te')
            elif(lang == 'Urdu'):
                text = translator.translator(text,'ur')
                audio_maker.make_aud(text,'ur')
            st.audio("res/temp.mp3")
            st.write(text)        
        except:
            st.write("Something went wrong. Try again")

st.write("\n")
if st.checkbox(" Read About Algorithm"):
    st.image(Image.open("res/Img_Desc.png"))
    st.markdown("##### Hear Article")
    st.audio("res/Img_Desc.mp3")
    st.markdown("##### Read Article")
    st.write('''Image captioning is the task of generating a natural language description of an image. The algorithm used by the Salesforce Blip Image Captioning Base model is a bootstrapping framework called BLIP. BLIP effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.

The BLIP model is a transformer-based model that is pre-trained on a massive dataset of images and captions. The model is able to learn the relationships between images and their captions, which allows it to generate accurate and informative captions for new images.

The BLIP model has been shown to achieve state-of-the-art results on a variety of image captioning benchmarks. The model is also able to generalize to new tasks, such as video captioning and VQA.

The following are the steps involved in the BLIP algorithm:

A captioner is trained to generate synthetic captions for a set of images.

A filter is trained to remove the noisy captions generated by the captioner.

The filtered captions are used to train a final image captioning model.

The BLIP algorithm has several advantages over other image captioning algorithms. First, the BLIP algorithm is able to effectively utilize noisy web data. This is because the filter is able to remove the noisy captions, which allows the final model to learn from the accurate captions. Second, the BLIP algorithm is able to generalize to new tasks. This is because the final model is trained on a diverse set of images and captions, which allows it to learn the relationships between images and their captions.

The Salesforce Blip Image Captioning Base model is a powerful tool for generating natural language descriptions of images. The model is able to achieve state-of-the-art results on a variety of image captioning benchmarks, and it is also able to generalize to new tasks. The BLIP algorithm is a promising new approach to image captioning, and it is likely to be further developed in the future.''')